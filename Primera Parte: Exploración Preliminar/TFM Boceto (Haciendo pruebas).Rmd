---
title: "TFM Boceto (Haciendo Pruebas)"
author: "Jaime Carreto Sánchez (jaime.carretos@um.es)"
date: "03/03/2025"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: true
    theme: spacelab
    toc: true
    toc_float: true
    css: styles.css
    keep_md: TRUE
  pdf_document:
    toc: true
subtitle: Máster en Bioinformática, Universidad de Murcia
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, progress = TRUE, verbose = TRUE)
```

## Introducción

El objetivo de este análisis es construir un modelo predictivo que identifique individuos con riesgo de desarrollar enfermedad coronaria a 10 años vista (`TenYearCHD`) a partir de variables clínicas disponibles en el conjunto de datos del *Framingham Heart Study*. Dado que esta enfermedad tiene un gran impacto en salud pública, identificar a tiempo a personas con riesgo puede ser crucial para aplicar medidas preventivas eficaces.

### Hipótesis de partida

Se espera que algunas variables clínicas como `male`, `age`, `education` y `BMI` tengan un poder descriptivo elevado, ya que actúan como **variables de confusión** (*confounding variables*). Estas son variables que están relacionadas tanto con los predictores como con la variable objetivo, pudiendo interferir en la capacidad del modelo para aprender relaciones verdaderamente causales si no se tienen en cuenta adecuadamente.

### Enfoque metodológico

Para evaluar el impacto de estas variables, se desarrollarán tres modelos de Random Forest utilizando `ranger`:

1. **Modelo M**: solo incluye las variables moleculares (`M`), es decir, todas las variables del dataset excluyendo `male`, `age`, `education` y `BMI`. Se espera que este modelo tenga la menor capacidad predictiva, al no incluir las variables clínicas más relevantes.

2. **Modelo MC**: utiliza todas las variables disponibles (`M + C`). Al incorporar las variables de confusión, se espera que mejore sustancialmente el rendimiento del modelo.

3. **Modelo M + always.split(C)**: incluye únicamente las variables `M`, pero forzando que cada una de las variables clínicas (`C`) se utilice como variable de división en los árboles de decisión mediante el parámetro `always.split.variables` del paquete `ranger`. Este parámetro permite que determinadas variables estén siempre disponibles como candidatas a dividir en cada nodo. El objetivo es evaluar si forzar el uso de estas variables críticas permite alcanzar un rendimiento similar (o incluso superior) al modelo MC, reduciendo además la complejidad computacional en escenarios con grandes volúmenes de datos.

Este diseño experimental permitirá evaluar no solo el impacto predictivo de las variables clínicas, sino también el valor práctico del parámetro `always.split.variables` en entornos reales.

# Carga de librerías y configuración inicial

```{r}
library(caret)
library(lattice)
library(ggplot2)
library(doParallel)
library(dplyr)
library(pROC)
library(DMwR2)
```

Defino mi directorio de trabajo:

```{r}
setwd("~/Escritorio/TFM/Primer_boceto")
```

Fijo una semilla aleatoria para garantizar la reproducibilidad

```{r}
set.seed(123)
```

# Carga y exploración inicial de los datos

```{r}
mydata <- read.csv("framingham.csv")
str(mydata)
```
El conjunto de datos contiene 4240 observaciones y 16 variables numéricas.

## Tratamiento de valores ausentes

Primero hay que comprobar la presencia de valores ausentes.

```{r}
colSums(is.na(mydata))
sum(is.na(mydata))
```
El Dataset presenta un total de 645 valores ausentes, concentrados especialmente en la variable `glucose`. Dado que eliminar las filas con `NA` supondría perder aproximadamente un 15% de los datos, optamos por imputar los valores ausentes mediante la mediana.

```{r}
preproc <- preProcess(mydata, method= "medianImpute")
mydata_imputado <- predict(preproc, newdata= mydata)
```

Ahora verifico que la imputación se ha realizado de manera efectiva

```{r}
sum(is.na(mydata_imputado))
```
Como se observa, el número de `NA` ahora es 0, por lo que la imputación se ha realizado de manera correcta.

## Preparación de los conjuntos de datos

Primero convierto la variable objetivo `TenYearCHD` en un factor

```{r}
mydata_imputado$TenYearCHD <- factor(mydata_imputado$TenYearCHD, levels = c(0, 1), labels = c("No", "Yes"))
table(mydata_imputado$TenYearCHD)
```

La variable a predecir `TenYearCHD` está muy desiquilibrada. El 85% de los datos representa la clase `No` mientras que el 15% restante es el `Sí` por lo tanto antes de entrenar los modelos habrá que hacer un downsampling de la clase mayoritaria para que el modelo aprenda de forma óptima.

Definimos los grupos de variables

```{r}
clase <- "TenYearCHD"
vars_C <- c("male", "age", "education", "BMI")
vars_M <- setdiff(names(mydata_imputado), c(vars_C, clase))
```

Ahora creo los diferentes Datasets y les añado la variable clase

```{r}
M <- mydata_imputado[,vars_M]
C <- mydata_imputado[,vars_C]
Y <- mydata_imputado[,clase]

df_M <- cbind(M, TenYearCHD= Y)
df_MC <- cbind(M, C, TenYearCHD= Y)
df_C <- cbind(C)
```

Primero divido los datos en entrenamiento y test:

```{r}
train_index <- createDataPartition(df_M$TenYearCHD, p=0.8, list=FALSE)
train_dataM <- df_M[train_index,]
test_dataM <- df_M[-train_index,]
```

Ahora como he explicado antes para que el entrenamiento del modelo se haga de manera adecuada voy a a balancear los datos usando la función `upSample` disponible en `caret`.

```{r}
train_dataM <- upSample(
  x=train_dataM[, -ncol(train_dataM)],
  y= train_dataM$TenYearCHD,
  yname= "TenYearCHD"
)
table(train_dataM$TenYearCHD)
```

El Upsampling se ha aplicado correctamente y ya tenemos los dos valores de la variable calase ajustados y listos para el entrenamiento.

Ahora creo una función para calcular el `Balanced Accuracy` que me servirá para todos los modelos

```{r}
balancedAccuracySummary <- function(data, lev= NULL, model = NULL) {
  cm <- caret::confusionMatrix(data$pred, data$obs)
  bal_acc <- cm$byClass["Balanced Accuracy"]
  out <- c(BalancedAccuracy = unname(bal_acc))
  return(out)
}
```

```{r}
tune_grid <- expand.grid(
  mtry = c(1,4,8,11),
  splitrule = "gini",
  min.node.size = c(10, 20, 30)
)

control <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = balancedAccuracySummary,
  classProbs = TRUE,
  verboseIter = FALSE)

modelo_M <- train(
  TenYearCHD ~ .,
  data = train_dataM,
  method = "ranger",
  tuneGrid = tune_grid,
  metric = "BalancedAccuracy",
  trControl = control
)
```

## Modelo Random Forest con la matriz MC

Divido los datos en entrenamiento y test

```{r}
train_index <- createDataPartition(df_MC$TenYearCHD, p= 0.8, list = FALSE)
train_dataMC <- df_MC[train_index,]
test_dataMC <- df_MC[-train_index,]
```

Una vez divididos hago el balanceo de la variable clase

```{r}
train_dataMC <- upSample(
  x=train_dataMC[, -ncol(train_dataMC)],
  y= train_dataMC$TenYearCHD,
  yname= "TenYearCHD"
)
table(train_dataMC$TenYearCHD)
```

```{r}
tune_grid <- expand.grid(
  mtry = c(1,2,4,8,11),
  splitrule = "gini",
  min.node.size = c(10, 20, 30)
)

control <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = balancedAccuracySummary,
  classProbs = TRUE,
  verboseIter = FALSE)

modelo_MC <- train(
  TenYearCHD ~ .,
  data = train_dataMC,
  method = "ranger",
  tuneGrid = tune_grid,
  metric = "BalancedAccuracy",
  trControl = control
)
```

## Modelo de Random Forest usando `always.split.variables`

```{r}
dataset_con_split <- list()

for (var_c in vars_C) {
  df_tmp <- cbind(
    M,
    var_c_value = mydata_imputado[[var_c]],
    TenYearCHD = mydata_imputado$TenYearCHD
  )
  colnames(df_tmp)[ncol(df_tmp)-1] <- var_c
  
  train_tmp <- df_tmp[train_index,]
  test_tmp <- df_tmp[-train_index,]
  
  train_tmp <- upSample(
    x= train_tmp[, -ncol(train_tmp)],
    y= train_tmp$TenYearCHD,
    yname = "TenYearCHD"
  )
  
  dataset_con_split[[var_c]] <- list(
    train= train_tmp,
    test= test_tmp
  )
}
table(train_tmp$TenYearCHD)
```

Como se observa, el balanceo se ha realizado de manera correcta. Ahora voy a pasar a entrenar al modelo.

```{r}
tune_grid <- expand.grid(
  mtry = c(1,2,4,8,11),
  splitrule = "gini",
  min.node.size = c(10, 20, 30)
)

control <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = balancedAccuracySummary,
  classProbs = TRUE,
  verboseIter = FALSE)

modelo_split <- list()
for (var_c in names(dataset_con_split)){

  modeloSplit <- train (
    TenYearCHD ~ .,
    data = dataset_con_split[[var_c]]$train,
    method="ranger",
    tuneGrid=tune_grid,
    trControl=control,
    metric="BalancedAccuracy",
    always.split.variables= var_c
  )
  
modelo_split[[var_c]] <- modeloSplit
}
```

# Resultados de los Modelos

## Resultados RF con Dataframe M

Imprimo los resultados del entrenamiento del modelo junto con los mejores hiperparámetros

```{r}
modelo_M$results
modelo_M$bestTune
```

Obtengo que los hiperparámetros que mejor resultado dan son `mtry`= 8 y `min.node.size`=10. Ahora voy a predecir sobre los datos de test, como estos están muy desbalanceados voy a cambiar el umbral de clasificación para elegir el que mejor resultado de **Balanced Accuracy** me de.

```{r}
probs_M <- predict(modelo_M, newdata = test_dataM, type = "prob")
thresholds <- seq(0.1, 0.9, by = 0.05)
balanced_accuracies <- c()

for (t in thresholds) {
  pred_classes <- ifelse(probs_M[,"Yes"] > t, "Yes", "No")
  pred_classes <- factor(pred_classes, levels = c("No", "Yes"))
  
  cm <- confusionMatrix(pred_classes, test_dataM$TenYearCHD, positive = "Yes")
  ba <- (cm$byClass["Sensitivity"] + cm$byClass["Specificity"]) / 2
  balanced_accuracies <- c(balanced_accuracies, ba)
}
best_threshold <- thresholds[which.max(balanced_accuracies)]
cat("Mejor umbral:", best_threshold, "\nBalanced Accuracy máxima:", max(balanced_accuracies), "\n")
```

En este caso el mejor umbral es 0.25, con un resultado de **Balanced Accuracy** de 0.57. Ahora observo los resultados de la matriz de confusión

```{r}
pred_opt <- ifelse(probs_M[,"Yes"] > best_threshold, "Yes", "No")
pred_opt <- factor(pred_opt, levels = c("No", "Yes"))
conf_opt <- confusionMatrix(pred_opt, test_dataM$TenYearCHD, positive = "Yes")
print(conf_opt)
```
Al **excluir las variables clínicas clave**, el modelo se queda con muy poca señal para distinguir entre individuos con y sin riesgo de enfermedad coronaria, por lo tanto su **capacidad práctica es muy limitada**. Este modelo funciona como buen punto de partida o control negativo, que ilustra lo que sucede cuando se ignorarn variables de confusión relevantes.

## Resultados RF con Dataframe MC

```{r}
modelo_MC$results
modelo_MC$bestTune
```

Los mejores hiperparámetros para el `modelo MC`s on `mtry`= 8 y `min.node.size` = 10. Ahora voy a predecir con esos parámetros sobre los datos de test, y como el problema está muy desbalanceado, cambio el umbral de clasificación y utilizo el que mejor resultado de **Balanced Accuracy**.

```{r}
probs_MC <- predict(modelo_MC, newdata = test_dataMC, type = "prob")
thresholds <- seq(0.1, 0.9, by = 0.05)
balanced_accuracies <- c()

for (t in thresholds) {
  pred_classes <- ifelse(probs_MC[,"Yes"] > t, "Yes", "No")
  pred_classes <- factor(pred_classes, levels = c("No", "Yes"))

  cm <- confusionMatrix(pred_classes, test_dataMC$TenYearCHD, positive = "Yes")
  ba <- (cm$byClass["Sensitivity"] + cm$byClass["Specificity"]) / 2
  balanced_accuracies <- c(balanced_accuracies, ba)
}
best_threshold_MC <- thresholds[which.max(balanced_accuracies)]
cat("Mejor umbral (modelo MC):", best_threshold_MC, "\nBalanced Accuracy máxima:", max(balanced_accuracies), "\n")
```

En este caso el mejor umbral es 0.2, con un resultado de **Balanced Accuracy** de 0.623, ahora imprimo la matriz de confusión.

```{r}
pred_MC_final <- ifelse(probs_MC[,"Yes"] > best_threshold_MC, "Yes", "No")
pred_MC_final <- factor(pred_MC_final, levels = c("No", "Yes"))
conf_MC_final <- confusionMatrix(pred_MC_final, test_dataMC$TenYearCHD, positive = "Yes")
print(conf_MC_final)
```
Incluir variables clínicas **mejora notablemente la sensibilidad**, que es el objetivo principal, detectar individuos en riesgo. Aunque la **Accuracy** baja respecto al modelo anterior, es normal porque el modelo ya no se limita solo a predecir la clase no. La **Balanced Accuracy** sube hasta el 0.62, lo que **demuestra un avance real en discriminación**. Por lo tanto, este modelo confirma la hipótesis inicial, incluir variables informativas **mejora sustancialmente** la capacidad predictiva de la clase minoritaria (YES).

## Resultados RF con `always.split.variables`

Primero voy a organizar los resultados de los 4 modelos en un mismo Dataframe para verlos mucho más claros

```{r}
resumen_modelos_split <- lapply(names(modelo_split), function(var_name) {
  modelo <- modelo_split[[var_name]]
  
  best <- modelo$results %>%
    filter(mtry == modelo$bestTune$mtry,
           min.node.size == modelo$bestTune$min.node.size)
  
  data.frame(
    variable_clinica = var_name,
    mtry = modelo$bestTune$mtry,
    min_node_size = modelo$bestTune$min.node.size,
    BalancedAccuracy = round(best$BalancedAccuracy, 4)
  )
}) %>% bind_rows()

resumen_modelos_split <- resumen_modelos_split %>% arrange(desc(BalancedAccuracy))
```

Ahora imprimo por pantalla los resultados organizados

```{r}
print(resumen_modelos_split)
```

```{r}
probs_edu <- predict(modelo_split[["education"]], newdata = dataset_con_split[["education"]]$test, type = "prob")

thresholds <- seq(0.1, 0.9, by = 0.05)
ba_edu <- c()

for (t in thresholds) {
  pred_classes <- ifelse(probs_edu[,"Yes"] > t, "Yes", "No")
  pred_classes <- factor(pred_classes, levels = c("No", "Yes"))
  ref <- dataset_con_split[["education"]]$test$TenYearCHD
  cm <- confusionMatrix(pred_classes, ref, positive = "Yes")
  ba <- (cm$byClass["Sensitivity"] + cm$byClass["Specificity"]) / 2
  ba_edu <- c(ba_edu, ba)
}

best_th_edu <- thresholds[which.max(ba_edu)]
cat("education, Mejor umbral:", best_th_edu, "- BA:", max(ba_edu), "\n")

pred_final_edu <- ifelse(probs_edu[,"Yes"] > best_th_edu, "Yes", "No")
pred_final_edu <- factor(pred_final_edu, levels = c("No", "Yes"))
conf_final_edu <- confusionMatrix(pred_final_edu, ref, positive = "Yes")
print(conf_final_edu)
```

Se observa una mejora clara respecto al modelo M, forzar `education` en los splits permite **capturar más casos positivos (mejora la sensibilidad)** sin introducir un gran deterioro general. Los resultados son cercanos al rendimiento del modelo MC, aunque sigue siendo algo inferior. Este resultado sugiere que `education` **contiene información relevante** para detectar pacientes en riesgo, pero por sí sola **no basta para igualar al modelo completo**.


```{r}
probs_age <- predict(modelo_split[["age"]], newdata = dataset_con_split[["age"]]$test, type = "prob")
thresholds <- seq(0.1, 0.9, by = 0.05)
ba_age <- c()
for (t in thresholds) {
  pred_classes <- ifelse(probs_age[,"Yes"] > t, "Yes", "No")
  pred_classes <- factor(pred_classes, levels = c("No", "Yes"))
  ref <- dataset_con_split[["age"]]$test$TenYearCHD
  cm <- confusionMatrix(pred_classes, ref, positive = "Yes")
  ba <- (cm$byClass["Sensitivity"] + cm$byClass["Specificity"]) / 2
  ba_age <- c(ba_age, ba)
}

best_th_age <- thresholds[which.max(ba_age)]
cat("Age, Mejor umbral:", best_th_age, "- BA:", max(ba_age), "\n")

pred_final_age <- ifelse(probs_age[,"Yes"] > best_th_age, "Yes", "No")
pred_final_age <- factor(pred_final_age, levels = c("No", "Yes"))
conf_final_age <- confusionMatrix(pred_final_age, ref, positive = "Yes")
print(conf_final_age)
```
Este modelo es algo **mas equilibrado y conservador**, sacrifica algo de sensibilidad respeccto al modelo completo para mantener mayor especificidad. Esto es **deseable en contextos donde los falsos positivos son costosos**. Forzar `age` en los splits permite un modelo más balanceado que el modelo M original, es más sensible que MC, pero más específico y por lo tanto muestra que la variable `age` tiene valor predictivo relevante y mejora el rendimiento sin necesidad de incluir otras variables clínicas.

```{r}
probs_BMI <- predict(modelo_split[["BMI"]], newdata = dataset_con_split[["BMI"]]$test, type = "prob")
thresholds <- seq(0.1, 0.9, by = 0.05)
ba_BMI <- c()
for (t in thresholds) {
  pred_classes <- ifelse(probs_BMI[,"Yes"] > t, "Yes", "No")
  pred_classes <- factor(pred_classes, levels = c("No", "Yes"))
  ref <- dataset_con_split[["BMI"]]$test$TenYearCHD
  cm <- confusionMatrix(pred_classes, ref, positive = "Yes")
  ba <- (cm$byClass["Sensitivity"] + cm$byClass["Specificity"]) / 2
  ba_BMI <- c(ba_BMI, ba)
}

best_th_BMI <- thresholds[which.max(ba_BMI)]
cat("BMI, Mejor umbral:", best_th_BMI, "- BA:", max(ba_BMI), "\n")

pred_final_BMI <- ifelse(probs_BMI[,"Yes"] > best_th_BMI, "Yes", "No")
pred_final_BMI <- factor(pred_final_BMI, levels = c("No", "Yes"))
conf_final_BMI <- confusionMatrix(pred_final_BMI, ref, positive = "Yes")
print(conf_final_BMI)
```

Este modelo **prioriza fuertemente la detección de casos positivos** con una **sensibilidad del 77%**, la más alta de todos los modelos. Sin embargo, lo hace a costa de una especificidad muy baja, lo que implica **muchos falsos positivos**. Por lo tanto es un modelo agresivo, tiende a etiquetar muchos casos como positivos, lo que puede ser útil si el objetivo es **maximizar la recall** pero no es adecuado si los falsos positivos tienen coste, como podría ser la realización de pruebas clínicas innecesarias.

```{r}
probs_male <- predict(modelo_split[["male"]], newdata = dataset_con_split[["male"]]$test, type = "prob")
thresholds <- seq(0.1, 0.9, by = 0.05)
ba_male <- c()
for (t in thresholds) {
  pred_classes <- ifelse(probs_male[,"Yes"] > t, "Yes", "No")
  pred_classes <- factor(pred_classes, levels = c("No", "Yes"))
  ref <- dataset_con_split[["male"]]$test$TenYearCHD
  cm <- confusionMatrix(pred_classes, ref, positive = "Yes")
  ba <- (cm$byClass["Sensitivity"] + cm$byClass["Specificity"]) / 2
  ba_male <- c(ba_male, ba)
}

best_th_male <- thresholds[which.max(ba_male)]
cat("Male, Mejor umbral:", best_th_male, "- BA:", max(ba_male), "\n")

pred_final_male <- ifelse(probs_male[,"Yes"] > best_th_male, "Yes", "No")
pred_final_male <- factor(pred_final_male, levels = c("No", "Yes"))
conf_final_male <- confusionMatrix(pred_final_male, ref, positive = "Yes")
print(conf_final_male)
```
Forzar la variable `male` como variable de split mejora significativamente la sensibilidad, lo que permite detectar hasta el 67% de los casos en riesgo. Aunque baja la especificidad, lo hace de forma más contenida que en el caso de BMI. La **Balanced Accuracy** de 0.6 indica un rendimiento equilibrado similar a los modelos con `education` o `age`. `Male` es una variable relevante para el riesgo cardiovascular, y **forzarla mejora el modelo respecto a M**. 